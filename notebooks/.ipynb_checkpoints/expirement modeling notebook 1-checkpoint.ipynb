{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import punkt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 30\n",
    "import lexnlp as lnlp\n",
    "import src\n",
    "from src import *\n",
    "import importlib\n",
    "import unidecode as unidecode\n",
    "importlib.reload(src)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352us282</td>\n",
       "      <td>may it please the court this case be here on a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353us586</td>\n",
       "      <td>mr chief justice if the court please when the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352us249</td>\n",
       "      <td>if the court please you might wait just a mome...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354us147</td>\n",
       "      <td>mr chief justice if the court please this be a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352us407</td>\n",
       "      <td>mr chief justice may it please the court this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case                                               text  target  \\\n",
       "0  352us282  may it please the court this case be here on a...       1   \n",
       "1  353us586  mr chief justice if the court please when the ...       1   \n",
       "2  352us249  if the court please you might wait just a mome...       0   \n",
       "3  354us147  mr chief justice if the court please this be a...       0   \n",
       "4  352us407  mr chief justice may it please the court this ...       1   \n",
       "\n",
       "   lib_or_con  majVotes  \n",
       "0         2.0         6  \n",
       "1         2.0         4  \n",
       "2         2.0         5  \n",
       "3         2.0         5  \n",
       "4         1.0         6  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_csv(\"../data/Final_Merge.csv\")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing all that sweet sweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9!]+')\n",
    "\n",
    "final_df.text = final_df.text.apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using my custom function to lemmatize my text data\n",
    "final_df.text = final_df.text.apply(lambda x: src.lemm_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352us282</td>\n",
       "      <td>[may, it, please, the, court, this, case, be, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353us586</td>\n",
       "      <td>[mr, chief, justice, if, the, court, please, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352us249</td>\n",
       "      <td>[if, the, court, please, you, might, wait, jus...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354us147</td>\n",
       "      <td>[mr, chief, justice, if, the, court, please, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352us407</td>\n",
       "      <td>[mr, chief, justice, may, it, please, the, cou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case                                               text  target  \\\n",
       "0  352us282  [may, it, please, the, court, this, case, be, ...       1   \n",
       "1  353us586  [mr, chief, justice, if, the, court, please, w...       1   \n",
       "2  352us249  [if, the, court, please, you, might, wait, jus...       0   \n",
       "3  354us147  [mr, chief, justice, if, the, court, please, t...       0   \n",
       "4  352us407  [mr, chief, justice, may, it, please, the, cou...       1   \n",
       "\n",
       "   lib_or_con  majVotes  \n",
       "0         2.0         6  \n",
       "1         2.0         4  \n",
       "2         2.0         5  \n",
       "3         2.0         5  \n",
       "4         1.0         6  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking my work\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fn_tdm_tfidf(docs, xColNames = None, **kwargs):\n",
    "    ''' create a term document matrix as pandas DataFrame\n",
    "    with **kwargs you can pass arguments of CountVectorizer\n",
    "    if xColNames is given the dataframe gets columns Names'''\n",
    "\n",
    "    #initialize the  vectorizer\n",
    "    tf = TfidfVectorizer(**kwargs)\n",
    "    x1 = tf.fit_transform(docs)\n",
    "    #create dataFrame\n",
    "    df = pd.DataFrame(x1.toarray().transpose(), index = tf.get_feature_names())\n",
    "\n",
    "    if xColNames is not None:\n",
    "        df.columns = xColNames\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = fn_tdm_tfidf(final_df.text).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aacp</th>\n",
       "      <th>aadc</th>\n",
       "      <th>aagency</th>\n",
       "      <th>aah</th>\n",
       "      <th>aals</th>\n",
       "      <th>aamc</th>\n",
       "      <th>aamr</th>\n",
       "      <th>aar</th>\n",
       "      <th>aaron</th>\n",
       "      <th>aaronson</th>\n",
       "      <th>aarp</th>\n",
       "      <th>aaup</th>\n",
       "      <th>ab</th>\n",
       "      <th>...</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zorach</th>\n",
       "      <th>zosianne</th>\n",
       "      <th>zschernig</th>\n",
       "      <th>zucca</th>\n",
       "      <th>zuckerman</th>\n",
       "      <th>zuckman</th>\n",
       "      <th>zur</th>\n",
       "      <th>zurcher</th>\n",
       "      <th>zurich</th>\n",
       "      <th>zweibaum</th>\n",
       "      <th>zweig</th>\n",
       "      <th>zwickler</th>\n",
       "      <th>zwiener</th>\n",
       "      <th>zwiezen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 37514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    aa  aaa  aacp  aadc  aagency  aah  aals  aamc  aamr  aar  aaron  aaronson  \\\n",
       "0  0.0  0.0   0.0   0.0      0.0  0.0   0.0   0.0   0.0  0.0    0.0       0.0   \n",
       "1  0.0  0.0   0.0   0.0      0.0  0.0   0.0   0.0   0.0  0.0    0.0       0.0   \n",
       "2  0.0  0.0   0.0   0.0      0.0  0.0   0.0   0.0   0.0  0.0    0.0       0.0   \n",
       "3  0.0  0.0   0.0   0.0      0.0  0.0   0.0   0.0   0.0  0.0    0.0       0.0   \n",
       "4  0.0  0.0   0.0   0.0      0.0  0.0   0.0   0.0   0.0  0.0    0.0       0.0   \n",
       "\n",
       "   aarp  aaup   ab  ...  zoom  zorach  zosianne  zschernig  zucca  zuckerman  \\\n",
       "0   0.0   0.0  0.0  ...   0.0     0.0       0.0        0.0    0.0        0.0   \n",
       "1   0.0   0.0  0.0  ...   0.0     0.0       0.0        0.0    0.0        0.0   \n",
       "2   0.0   0.0  0.0  ...   0.0     0.0       0.0        0.0    0.0        0.0   \n",
       "3   0.0   0.0  0.0  ...   0.0     0.0       0.0        0.0    0.0        0.0   \n",
       "4   0.0   0.0  0.0  ...   0.0     0.0       0.0        0.0    0.0        0.0   \n",
       "\n",
       "   zuckman  zur  zurcher  zurich  zweibaum  zweig  zwickler  zwiener  zwiezen  \n",
       "0      0.0  0.0      0.0     0.0       0.0    0.0       0.0      0.0      0.0  \n",
       "1      0.0  0.0      0.0     0.0       0.0    0.0       0.0      0.0      0.0  \n",
       "2      0.0  0.0      0.0     0.0       0.0    0.0       0.0      0.0      0.0  \n",
       "3      0.0  0.0      0.0     0.0       0.0    0.0       0.0      0.0      0.0  \n",
       "4      0.0  0.0      0.0     0.0       0.0    0.0       0.0      0.0      0.0  \n",
       "\n",
       "[5 rows x 37514 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop('text', axis = 1)\n",
    "df3 = df2.merge(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('target', axis = 1)\n",
    "Y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X, Y, test_size=0.6, random_state=34)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using accuracy as my metric as this model will have little actionable impact, its not important to maximise other metrics. The more accurate the model the better a data point it will be for legal prognosticators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and vectorizing with TFIDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used sklearn to get the tfidf scores for my data. I tried a couple of different ngram ranges and 1-3 seemed to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the tfidf model as I know it, I rejoined the lemmatized words into one string\n",
    "final_df.text = final_df.text.apply(lambda x: ','.join(x)) \n",
    "final_df.text =final_df.text.apply(lambda x: x.replace(',',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.text#assigning my features\n",
    "Y = final_df.target#and my targets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating stopwords, I included some legal ones, I could have done this in my src file \n",
    "#and imported them, but it was easier to expirement with different options by generating\n",
    "#them within the notebook\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',\n",
    "        'said', 'one', 'com','-', '–', '—', 'co', 'wa', 'ha', '1', 'amp',\n",
    "        'court', 'would', 'case', 'say', 'think']\n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X, Y, test_size=0.6, random_state=34)\n",
    "#vectorizing with TFIDF scores\n",
    "tfidf = TfidfVectorizer(ngram_range= (1,3), stop_words= sw_set)\n",
    "#transforming the data \n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top ten TFIDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zwiener question obscenity', 'forbid kind evidence', 'forbid free', 'forbid form desecration', 'forbid form', 'forbid forbid special', 'forbid forbid district', 'forbid first amendment', 'forbid first', 'forbid federal constitution']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(tfidf.idf_)[::-1]\n",
    "features = tfidf.get_feature_names()\n",
    "top_n = 10\n",
    "top_features = [features[i] for i in indices[:top_n]]\n",
    "print (top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untuned RFC (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use an RFC as my baseline model. It's generally good for NLP classification tasks and not overly difficult to implement untuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the model\n",
    "rf_classifier_lem = RandomForestClassifier(n_estimators=10, random_state=0, class_weight= 'balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "#generating test predictions\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)\n",
    "#generating train predictions\n",
    "rf_train_preds_lem = rf_classifier_lem.predict(tfidf_data_train_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulating test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Lemmatization Features\n",
      "Testing Accuracy: 0.5448\n",
      "\n",
      "F1 Score: 0.6074\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "rf_acc_score_lem = accuracy_score(y_test_lem, rf_test_preds_lem)#accuracy score\n",
    "rf_f1_score_lem = f1_score(y_test_lem, rf_test_preds_lem)#F1 score\n",
    "print('Random Forest with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score_lem))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 519\n",
      "True Negatives: 284\n",
      " False Positives: 318\n",
      " False Negatives: 353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, rf_test_preds_lem)#generating a confusion matrix\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "#printing it in the format which I find most readable, rather than the graphic version\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulating training predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Lemmatization Features\n",
      "Testing Accuracy: 0.9868\n",
      "\n",
      "F1 Score: 0.9885\n"
     ]
    }
   ],
   "source": [
    "rf_acc_score_lem_t = accuracy_score(y_train_lem, rf_train_preds_lem)#accuracy score\n",
    "rf_f1_score_lem_t = f1_score(y_train_lem, rf_train_preds_lem)# F1 score\n",
    "print('Random Forest with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score_lem_t))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score_lem_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 557\n",
      "True Negatives: 412\n",
      " False Positives: 7\n",
      " False Negatives: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, rf_train_preds_lem) #generating a confusion matrix\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "#printing it in the format which I find most readable, rather than the graphic version\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the baseline model is way way way overfit, I will try some different models and see if they have similar problems, then try a gridsearch cv to try to adress this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB(alpha = .1)#instantiating a multinomial naive bayes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.fit(tfidf_data_train_lem, y_train_lem) #fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_preds_lem = nb_classifier.predict(tfidf_data_test_lem)\n",
    "nb_train_preds_lem = nb_classifier.predict(tfidf_data_train_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_acc_score_lem = accuracy_score(y_test_lem, nb_test_preds_lem)\n",
    "nb_f1_score_lem = f1_score(y_test_lem, nb_test_preds_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Lemmatization Features\n",
      "Testing Accuracy: 0.5909\n",
      "\n",
      "F1 Score: 0.7429\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(nb_acc_score_lem))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(nb_f1_score_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model did a little better on the testing data but not a by a huge margin. It is about the same as predicting the dominant class wil always win (petitioner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 519\n",
      "True Negatives: 284\n",
      " False Positives: 318\n",
      " False Negatives: 353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, rf_test_preds_lem)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_acc_score_lem_t = accuracy_score(y_train_lem, nb_train_preds_lem)\n",
    "nb_f1_score_lem_t = f1_score(y_train_lem, nb_train_preds_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Lemmatization Features\n",
      "Testing Accuracy: 1.0\n",
      "\n",
      "F1 Score: 0.7429\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(nb_acc_score_lem_t))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(nb_f1_score_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 563\n",
      "True Negatives: 419\n",
      " False Positives: 0\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, nb_train_preds_lem)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still having the same drastic overfitting issues with this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ten Most Imporant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honor make right well state think say would case court\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top10 = np.argsort(nb_classifier.coef_[0])[-10:]\n",
    "\n",
    "print(\" \".join(feature_names[j] for j in top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are largely the same as the most common words I found in my EDA. I thought that with TFIDF scores and modeling there would be more differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont have high expectations but lets see if a grid search RCF can do any better before we fully decide on using the NB classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': range(2,10),\n",
    "    'max_features': ['auto']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree =GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': range(2, 10),\n",
       "                         'max_features': ['auto'],\n",
       "                         'n_estimators': [10, 20, 50, 100]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "grfc_preds_test =grid_tree.predict(tfidf_data_test_lem)\n",
    "grfc_preds_train = grid_tree.predict(tfidf_data_train_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_test = accuracy_score(grfc_preds, y_test_lem)\n",
    "f1_score_test = f1_score(grfc_preds, y_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched Random Forest Classifier with Lemmatization Features\n",
      "Testing Accuracy: 0.5813\n",
      "\n",
      "F1 Score: 0.7352\n"
     ]
    }
   ],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy_score_test))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(f1_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 286\n",
      "True Negatives: 0\n",
      " False Positives: 206\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, grfc_preds_test)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this model is only predicting the dominant class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_train = accuracy_score(grfc_preds_train, y_train_lem)\n",
    "f1_score_train = f1_score(grfc_preds_train, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched Random Forest Classifier with Lemmatization Features\n",
      "Testing Accuracy: 0.585\n",
      "\n",
      "F1 Score: 0.7382\n"
     ]
    }
   ],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy_score_train))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(f1_score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1149\n",
      "True Negatives: 0\n",
      " False Positives: 815\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, grfc_preds_train)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does the same on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 20}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the grid search proccess adressed the overfitting by increasing the bias by lowering the max depth and estimators. This eventually must have led to just predicting that the petitioner would win"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X= tfidf_data_train_lem, y= y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_preds = svc.predict(tfidf_data_test_lem)\n",
    "SVC_preds_train = svc.predict(tfidf_data_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5630936227951153"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SVC_preds, y_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(SVC_preds_train, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is the best, in fact its 1% more accurate then just guessing that the petitioner won! Not great but since it seems the most promising I will try tuning its hyperparameters and see if I can squeeze some more juice out of it and cross validate my results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_gridsvc = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_svc = GridSearchCV(SVC(), param_gridsvc, cv=3, scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 18.5min\n",
      "[Parallel(n_jobs=-1)]: Done 144 out of 144 | elapsed: 62.0min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, estimator=SVC(), n_jobs=-1,\n",
       "             param_grid={'C': [0.1, 1, 10, 100], 'gamma': [1, 0.1, 0.01, 0.001],\n",
       "                         'kernel': ['rbf', 'poly', 'sigmoid']},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svc.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "predsvc = grid_svc.predict(tfidf_data_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5813008130081301"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(predsvc, y_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many roads lead to the same place. I think I will stick with the nb classifier at this point as it is fast and competitive, but none of the models are amazing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_clf = xgb.XGBClassifier(objective ='binary:logistic', \n",
    "                           colsample_bytree = 0.5, \n",
    "                           subsample = 0.5,\n",
    "                           learning_rate = 0.1,\n",
    "                           max_depth = 2, \n",
    "                           alpha = 1, \n",
    "                           n_estimators = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(alpha=1, colsample_bytree=0.5, max_depth=2, n_estimators=10,\n",
       "              subsample=0.5)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xg_clf.fit(tfidf_data_train_lem , y_train_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_test = xg_clf.predict(tfidf_data_test_lem)\n",
    "xg_train = xg_clf.predict(tfidf_data_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_accuracy = accuracy_score(y_train_lem, xg_train)\n",
    "xg_f1 = f1_score(y_test_lem, xg_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Lemmatization Features\n",
      "Testing Accuracy: 1.0\n",
      "\n",
      "F1 Score: 0.6591\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(xg_accuracy))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(xg_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if using W2V and getting the mean vector for each argument leads to a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = df.text.map(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237773039, 795415180)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(vocab, size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "model.train(vocab, total_examples=model.corpus_count, epochs= 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidancoco/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wtv = dict(zip(model.wv.index2word, model.wv.syn0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "class W2vVectorizer(object):\n",
    "    \n",
    "    def __init__(self, w2v):\n",
    "        # Takes in a dictionary of words and vectors as input\n",
    "        self.w2v = w2v\n",
    "        if len(w2v) == 0:\n",
    "            self.dimensions = 0\n",
    "        else:\n",
    "            self.dimensions = len(w2v[next(iter(wtv))])\n",
    "    \n",
    "    # Note: Even though it doesn't do anything, it's required that this object implement a fit method or else\n",
    "    # it can't be used in a scikit-learn pipeline  \n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return np.array([\n",
    "            np.mean([self.w2v[w] for w in words if w in self.w2v]\n",
    "                   or [np.zeros(self.dimensions)], axis=0) for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', W2vVectorizer(wtv)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(wtv)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', W2vVectorizer(wtv)),\n",
    "              ('Logistic Regression', LogisticRegression(max_iter= 1000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, final_df.text, final_df['target'], cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.5565960912052117),\n",
       " ('Support Vector Machine', 0.5842833876221498),\n",
       " ('Logistic Regression', 0.5842833876221498)]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgr = LogisticRegression(class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W2V vectors are not any better than using TFIDF scores and involve a more complicated pipeline proccess so I would still recommend using the NB classifier model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these models trained on my language data worked rather poorly. While this is a difficult thing to predict, there was no improvement over simply guessing the dominant class everytime. I would not reccomend using NLP to predict Supreme Court Cases. Other researchers have had far more success using other factors of the case in machine learning algorithims. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making the Prediction From my NLP Model into a DataFrame to Use With Non NLP Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = tfidf.transform(X)#trasforming all the data with tfidf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = nb_classifier.predict_proba(X_all)#predicting with the classifier and calling the probality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = list(probs) #turnin them into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = pd.DataFrame(probs)#turning them into a DataFrame\n",
    "df_probs.columns = ['pwin', 'ploss'] #column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352us282</td>\n",
       "      <td>may it please the court this case be here on a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353us586</td>\n",
       "      <td>mr chief justice if the court please when the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352us249</td>\n",
       "      <td>if the court please you might wait just a mome...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354us147</td>\n",
       "      <td>mr chief justice if the court please this be a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352us407</td>\n",
       "      <td>mr chief justice may it please the court this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case                                               text  target  \\\n",
       "0  352us282  may it please the court this case be here on a...       1   \n",
       "1  353us586  mr chief justice if the court please when the ...       1   \n",
       "2  352us249  if the court please you might wait just a mome...       0   \n",
       "3  354us147  mr chief justice if the court please this be a...       0   \n",
       "4  352us407  mr chief justice may it please the court this ...       1   \n",
       "\n",
       "   lib_or_con  majVotes  \n",
       "0         2.0         6  \n",
       "1         2.0         4  \n",
       "2         2.0         5  \n",
       "3         2.0         5  \n",
       "4         1.0         6  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs = df_probs.merge(final_df, left_index = True, right_index= True) #merging with the prior DF, the order is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pwin</th>\n",
       "      <th>ploss</th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.023292</td>\n",
       "      <td>0.976708</td>\n",
       "      <td>352us282</td>\n",
       "      <td>may it please the court this case be here on a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.008857</td>\n",
       "      <td>0.991143</td>\n",
       "      <td>353us586</td>\n",
       "      <td>mr chief justice if the court please when the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.991100</td>\n",
       "      <td>0.008900</td>\n",
       "      <td>352us249</td>\n",
       "      <td>if the court please you might wait just a mome...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.940557</td>\n",
       "      <td>0.059443</td>\n",
       "      <td>354us147</td>\n",
       "      <td>mr chief justice if the court please this be a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.110519</td>\n",
       "      <td>0.889481</td>\n",
       "      <td>352us407</td>\n",
       "      <td>mr chief justice may it please the court this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       pwin     ploss      case  \\\n",
       "0  0.023292  0.976708  352us282   \n",
       "1  0.008857  0.991143  353us586   \n",
       "2  0.991100  0.008900  352us249   \n",
       "3  0.940557  0.059443  354us147   \n",
       "4  0.110519  0.889481  352us407   \n",
       "\n",
       "                                                text  target  lib_or_con  \\\n",
       "0  may it please the court this case be here on a...       1         2.0   \n",
       "1  mr chief justice if the court please when the ...       1         2.0   \n",
       "2  if the court please you might wait just a mome...       0         2.0   \n",
       "3  mr chief justice if the court please this be a...       0         2.0   \n",
       "4  mr chief justice may it please the court this ...       1         1.0   \n",
       "\n",
       "   majVotes  \n",
       "0         6  \n",
       "1         4  \n",
       "2         5  \n",
       "3         5  \n",
       "4         6  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_probs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs =df_probs[['case', 'pwin', 'ploss']] #getting only the columns I want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_probs.to_csv('probs_case.csv', index = False)#exporting as a csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
