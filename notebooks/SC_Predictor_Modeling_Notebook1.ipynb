{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.tokenize import punkt\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.metrics import recall_score, accuracy_score, confusion_matrix\n",
    "import string\n",
    "from nltk.probability import FreqDist\n",
    "import seaborn as sns\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_columns = 30\n",
    "import lexnlp as lnlp\n",
    "import src\n",
    "from src import *\n",
    "import importlib\n",
    "import unidecode as unidecode\n",
    "importlib.reload(src)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Feature Engineering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352us282</td>\n",
       "      <td>may it please the court this case be here on a...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353us586</td>\n",
       "      <td>mr chief justice if the court please when the ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352us249</td>\n",
       "      <td>if the court please you might wait just a mome...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354us147</td>\n",
       "      <td>mr chief justice if the court please this be a...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352us407</td>\n",
       "      <td>mr chief justice may it please the court this ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case                                               text  target  \\\n",
       "0  352us282  may it please the court this case be here on a...       1   \n",
       "1  353us586  mr chief justice if the court please when the ...       1   \n",
       "2  352us249  if the court please you might wait just a mome...       0   \n",
       "3  354us147  mr chief justice if the court please this be a...       0   \n",
       "4  352us407  mr chief justice may it please the court this ...       1   \n",
       "\n",
       "   lib_or_con  majVotes  \n",
       "0         2.0         6  \n",
       "1         2.0         4  \n",
       "2         2.0         5  \n",
       "3         2.0         5  \n",
       "4         1.0         6  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df = pd.read_csv(\"../data/Final_Merge.csv\")#importing the text data I cleaned\n",
    "final_df.head()#checking it out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing all that sweet sweet text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9!]+')#instaniating a reg ex tokenizer\n",
    "\n",
    "final_df.text = final_df.text.apply(lambda x: tokenizer.tokenize(x)) #applying it to the text column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a custom function to lemmatize my text data\n",
    "final_df.text = final_df.text.apply(lambda x: src.lemm_text(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>case</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>lib_or_con</th>\n",
       "      <th>majVotes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>352us282</td>\n",
       "      <td>[may, it, please, the, court, this, case, be, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>353us586</td>\n",
       "      <td>[mr, chief, justice, if, the, court, please, w...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>352us249</td>\n",
       "      <td>[if, the, court, please, you, might, wait, jus...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>354us147</td>\n",
       "      <td>[mr, chief, justice, if, the, court, please, t...</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>352us407</td>\n",
       "      <td>[mr, chief, justice, may, it, please, the, cou...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       case                                               text  target  \\\n",
       "0  352us282  [may, it, please, the, court, this, case, be, ...       1   \n",
       "1  353us586  [mr, chief, justice, if, the, court, please, w...       1   \n",
       "2  352us249  [if, the, court, please, you, might, wait, jus...       0   \n",
       "3  354us147  [mr, chief, justice, if, the, court, please, t...       0   \n",
       "4  352us407  [mr, chief, justice, may, it, please, the, cou...       1   \n",
       "\n",
       "   lib_or_con  majVotes  \n",
       "0         2.0         6  \n",
       "1         2.0         4  \n",
       "2         2.0         5  \n",
       "3         2.0         5  \n",
       "4         1.0         6  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking my work\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using accuracy as my metric as this model will have little actionable impact, its not important to maximise other metrics. The more accurate the model the better a data point it will be for legal prognosticators."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split and vectorizing with TFIDF scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used sklearn to get the tfidf scores for my data. I tried a couple of different ngram ranges and 1-3 seemed to be the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to use the tfidf model as I know it, I rejoined the lemmatized words into one string\n",
    "final_df.text = final_df.text.apply(lambda x: ','.join(x)) \n",
    "final_df.text =final_df.text.apply(lambda x: x.replace(',',' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = final_df.text#assigning my features\n",
    "Y = final_df.target#and my targets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating stopwords, I included some legal ones, I could have done this in my src file \n",
    "#and imported them, but it was easier to expirement with different options by generating\n",
    "#them within the notebook\n",
    "sw_list = stopwords.words('english')\n",
    "sw_list += list(string.punctuation)\n",
    "sw_list += [\"''\", '\"\"', '...', '``', '’', '“', '’', '”', '‘', '‘', '©',\n",
    "        'said', 'one', 'com','-', '–', '—', 'co', 'wa', 'ha', '1', 'amp',\n",
    "        'court', 'would', 'case', 'say', 'think''may', 'it', 'please', 'the', 'court', 'justice', 'thank', 'you', 'mrs', 'chief', 'honor'] \n",
    "sw_set = set(sw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train_lem, X_test_lem, y_train_lem, y_test_lem = train_test_split(X, Y, test_size=0.6, random_state=34)\n",
    "#vectorizing with TFIDF scores\n",
    "tfidf = TfidfVectorizer(ngram_range= (1,3), stop_words= sw_set)#I tried different ranges of ngrams and 1-3 was the best\n",
    "#transforming the data \n",
    "tfidf_data_train_lem = tfidf.fit_transform(X_train_lem)\n",
    "tfidf_data_test_lem = tfidf.transform(X_test_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top ten TFIDF scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['zwiener question obscenity', 'ford versus wainwright', 'ford summary underlying', 'ford summary reason', 'ford summary', 'ford summarize overall', 'ford summarize', 'ford suggest whether', 'ford suggest', 'ford submit rule']\n"
     ]
    }
   ],
   "source": [
    "indices = np.argsort(tfidf.idf_)[::-1]#grabbing the top ten tfidf score indices\n",
    "features = tfidf.get_feature_names() #getting the feature names\n",
    "top_n = 10\n",
    "top_features = [features[i] for i in indices[:top_n]] # matching them up \n",
    "print (top_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main interesting word here is forbid, it's featured in nine of the top ten tri grams, bigrams or words. Furthermore, almost every high tfidf word is a trigram with only three bigrams in the top ten. Obscenity and free speech show up which are interrelated issues and were also more likely to be winning issues for the petitioners. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Untuned RFC (baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use an RFC as my baseline model. It's generally good for NLP classification tasks and not overly difficult to implement untuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiating the model\n",
    "rf_classifier_lem = RandomForestClassifier(n_estimators=10, random_state=0, class_weight= 'balanced')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting\n",
    "rf_classifier_lem.fit(tfidf_data_train_lem, y_train_lem)\n",
    "#generating test predictions\n",
    "rf_test_preds_lem = rf_classifier_lem.predict(tfidf_data_test_lem)\n",
    "#generating train predictions\n",
    "rf_train_preds_lem = rf_classifier_lem.predict(tfidf_data_train_lem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulating test predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Lemmatization Features\n",
      "Testing Accuracy: 0.5136\n",
      "\n",
      "F1 Score: 0.5625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "rf_acc_score_lem = accuracy_score(y_test_lem, rf_test_preds_lem)#accuracy score\n",
    "rf_f1_score_lem = f1_score(y_test_lem, rf_test_preds_lem)#F1 score\n",
    "print('Random Forest with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score_lem))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 461\n",
      "True Negatives: 296\n",
      " False Positives: 306\n",
      " False Negatives: 411\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, rf_test_preds_lem)#generating a confusion matrix\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "#printing it in the format which I find most readable, rather than the graphic version\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaulating training predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Lemmatization Features\n",
      "Testing Accuracy: 0.9878\n",
      "\n",
      "F1 Score: 0.9894\n"
     ]
    }
   ],
   "source": [
    "rf_acc_score_lem_t = accuracy_score(y_train_lem, rf_train_preds_lem)#accuracy score\n",
    "rf_f1_score_lem_t = f1_score(y_train_lem, rf_train_preds_lem)# F1 score\n",
    "print('Random Forest with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(rf_acc_score_lem_t))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(rf_f1_score_lem_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 558\n",
      "True Negatives: 412\n",
      " False Positives: 7\n",
      " False Negatives: 5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, rf_train_preds_lem) #generating a confusion matrix\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "#printing it in the format which I find most readable, rather than the graphic version\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like the baseline model is way way way overfit, I will try some different models and see if they have similar problems, then try a gridsearch cv to try to adress this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classifier = MultinomialNB(alpha = .1)#instantiating a multinomial naive bayes model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=0.1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_classifier.fit(tfidf_data_train_lem, y_train_lem) #fitting\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_test_preds_lem = nb_classifier.predict(tfidf_data_test_lem)\n",
    "nb_train_preds_lem = nb_classifier.predict(tfidf_data_train_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_acc_score_lem = accuracy_score(y_test_lem, nb_test_preds_lem)\n",
    "nb_f1_score_lem = f1_score(y_test_lem, nb_test_preds_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Lemmatization Features\n",
      "Testing Accuracy: 0.5909\n",
      "\n",
      "F1 Score: 0.7429\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(nb_acc_score_lem))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(nb_f1_score_lem))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Naive Bayes model did a little better on the testing data but not a by a huge margin. It is about the same as predicting the dominant class wil always win (petitioner. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 519\n",
      "True Negatives: 284\n",
      " False Positives: 318\n",
      " False Negatives: 353\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, rf_test_preds_lem)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_acc_score_lem_t = accuracy_score(y_train_lem, nb_train_preds_lem)\n",
    "nb_f1_score_lem_t = f1_score(y_train_lem, nb_train_preds_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes with Lemmatization Features\n",
      "Testing Accuracy: 1.0\n",
      "\n",
      "F1 Score: 0.7429\n"
     ]
    }
   ],
   "source": [
    "print('Naive Bayes with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(nb_acc_score_lem_t))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(nb_f1_score_lem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 563\n",
      "True Negatives: 419\n",
      " False Positives: 0\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, nb_train_preds_lem)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still having the same drastic overfitting issues with this model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Ten Most Imporant Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "honor make right well state think say would case court\n"
     ]
    }
   ],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "top10 = np.argsort(nb_classifier.coef_[0])[-10:]\n",
    "\n",
    "print(\" \".join(feature_names[j] for j in top10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are largely the same as the most common words I found in my EDA. I thought that with TFIDF scores and modeling there would be more differences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuned RFC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I dont have high expectations but lets see if a grid search RCF can do any better before we fully decide on using the NB classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { \n",
    "    'n_estimators': [10, 20, 50, 100],\n",
    "    'criterion': ['gini'],\n",
    "    'max_depth': range(2,10),\n",
    "    'max_features': ['auto']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_tree =GridSearchCV(RandomForestClassifier(), param_grid, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 160 out of 160 | elapsed:  8.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, estimator=RandomForestClassifier(), n_jobs=-1,\n",
       "             param_grid={'criterion': ['gini'], 'max_depth': range(2, 10),\n",
       "                         'max_features': ['auto'],\n",
       "                         'n_estimators': [10, 20, 50, 100]},\n",
       "             scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.fit(tfidf_data_train_lem, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "grfc_preds_test =grid_tree.predict(tfidf_data_test_lem)\n",
    "grfc_preds_train = grid_tree.predict(tfidf_data_train_lem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_test = accuracy_score(grfc_preds, y_test_lem)\n",
    "f1_score_test = f1_score(grfc_preds, y_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched Random Forest Classifier with Lemmatization Features\n",
      "Testing Accuracy: 0.5813\n",
      "\n",
      "F1 Score: 0.7352\n"
     ]
    }
   ],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy_score_test))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(f1_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 286\n",
      "True Negatives: 0\n",
      " False Positives: 206\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, grfc_preds_test)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately this model is only predicting the dominant class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score_train = accuracy_score(grfc_preds_train, y_train_lem)\n",
    "f1_score_train = f1_score(grfc_preds_train, y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched Random Forest Classifier with Lemmatization Features\n",
      "Testing Accuracy: 0.585\n",
      "\n",
      "F1 Score: 0.7382\n"
     ]
    }
   ],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy_score_train))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(f1_score_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 1149\n",
      "True Negatives: 0\n",
      " False Positives: 815\n",
      " False Negatives: 0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_train_lem, grfc_preds_train)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does the same on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'gini',\n",
       " 'max_depth': 3,\n",
       " 'max_features': 'auto',\n",
       " 'n_estimators': 20}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_tree.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the grid search proccess adressed the overfitting by increasing the bias by lowering the max depth and estimators. This eventually must have led to just predicting that the petitioner would win, This indicates to me that modeling based on text data may be pretty much impossible without a sequential nueral net. I will try a few more models with the straight up TFIDF score but I do not think they will be any better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(class_weight= 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(class_weight='balanced')"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X= tfidf_data_train_lem, y= y_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "SVC_preds_test = svc.predict(tfidf_data_test_lem)\n",
    "SVC_preds_train = svc.predict(tfidf_data_train_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_acc_test = accuracy_score(SVC_preds_test, y_test_lem)\n",
    "svc_f1_test = f1_score(SVC_preds_test, y_test_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid searched Random Forest Classifier with Lemmatization Features\n",
      "Testing Accuracy: 0.5672\n",
      "\n",
      "F1 Score: 0.6577\n"
     ]
    }
   ],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(svc_acc_test))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(svc_f1_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Positives: 613\n",
      "True Negatives: 223\n",
      " False Positives: 379\n",
      " False Negatives: 259\n",
      "\n"
     ]
    }
   ],
   "source": [
    "confusion = confusion_matrix(y_test_lem, SVC_preds_test)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print( f'True Positives: {TP}\\n'\n",
    "       f'True Negatives: {TN}\\n', \n",
    "      f'False Positives: {FP}\\n',\n",
    "      f'False Negatives: {FN}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Grid searched Random Forest Classifier with Lemmatization Features')\n",
    "print(\"Testing Accuracy: {:.4}\".format(accuracy_score_test))\n",
    "print()\n",
    "print(\"F1 Score: {:.4}\".format(f1_score_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many roads lead to the same place. Every model is fairly overfit and the optimized random forest classifier which I gridsearched finds it best to just guess that the petitioner will win 100% of the cases. I'm unsure exactly why the overfitting is happenening, it might be due to the curse of dimensionality and the multitude of words within the dataset. XG boost models are supposed to be good at adressing overfitting so I will give that a shot then give up on these more straight forward TFIDF models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# W2V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see if using W2V and getting the mean vector for each argument leads to a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = df.text.map(word_tokenize) # generating all my tokens in a format good for Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(237773039, 795415180)"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec(vocab, size=100, window=5, min_count=1, workers=4) #generating the model \n",
    "\n",
    "model.train(vocab, total_examples=model.corpus_count, epochs= 10) # training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aidancoco/anaconda3/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "wtv = dict(zip(model.wv.index2word, model.wv.syn0)) #making a dictionary for each words vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I will use a use a custom word2vecVectorizer class than can get the mean vector for each case then implement that in a pipeline \n",
    "#three different models, an RFC, a SVM and a simple Logistic Regression, then display the mean cross val score for\n",
    "#each model over 5 folds\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf =  Pipeline([('Word2Vec Vectorizer', src.W2vVectorizer(wtv)),\n",
    "              ('Random Forest', RandomForestClassifier(n_estimators=100, verbose=True))])\n",
    "svc = Pipeline([('Word2Vec Vectorizer', src.W2vVectorizer(wtv)),\n",
    "                ('Support Vector Machine', SVC())])\n",
    "lr = Pipeline([('Word2Vec Vectorizer', srr.W2vVectorizer(wtv)),\n",
    "              ('Logistic Regression', LogisticRegression(max_iter= 1000))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [('Random Forest', rf),\n",
    "          ('Support Vector Machine', svc),\n",
    "          ('Logistic Regression', lr)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    1.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 100 out of 100 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "scores = [(name, cross_val_score(model, final_df.text, final_df['target'], cv=2).mean()) for name, model, in models]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Random Forest', 0.5565960912052117),\n",
       " ('Support Vector Machine', 0.5842833876221498),\n",
       " ('Logistic Regression', 0.5842833876221498)]"
      ]
     },
     "execution_count": 501,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I would do more to cross validate these models, but I am starting to get deja vu from these scores, they all hit a hard limit right around where always guessing the dominant class would get you in terms of accuracy. The SVM with the W2V mean vectors for each case does the \"best\" but is still not good or worth pursuing from an analysis perspective."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All these models trained on my language data worked rather poorly. While this is a difficult thing to predict, there was no improvement over simply guessing the dominant class everytime. I would not reccomend using NLP to predict Supreme Court Cases. Other researchers have had far more success using other factors of the case in machine learning algorithims. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "332.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
